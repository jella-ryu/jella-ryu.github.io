---
layout: post
title: "분류 모델 평가 지표"
category: "Machine Learning" 
tags: [ML, Classification]
comments: true
published: true
---

> 📚 Precision, Recall, F1 Score 등 분류 모델의 성능 지표 중 무엇을 써야할까?

### 혼동 행렬 (Confusion Matrix)

- 분류 모델의 예측 결과를 표 형태로 요약한 것.
    - 행: 실제 값 (True Class)
    - 열: 예측 값 (Predicted Class)

![Confusion Matrix](/images/confusion_matrix.png)

### 1. Precision (정밀도)

- 정의: 모델이 양성(Positive)으로 예측한 사례 중에서 실제로 양성인 비율
- 측정 방법: 모델이 **양성으로 예측한 사례 중**에서 **실제로 양성인 비율**을 측정합니다.
- 의미: "양성이라고 예측한 결과가 얼마나 정확한가?"를 측정합니다. 따라서 False Positive(잘못된 양성 예측)를 줄이는 데 초점을 두고 있습니다.
- 활용 목적: 자원이나 비용이 한정되어 있을 때 중요합니다.

| 예시 | 설명 |
|:------:|:------|
| 예1 | 스팸 필터에서 Precision이 높으면 진짜 스팸만 걸러낼 가능성이 높음 |
| 예2 | 광고로 전환 가능성이 높은 사용자를 선택(분류)해야 하므로 Precision을 높이는 것이 중요 |

---

### 2. Recall (재현율)
- 정의: 실제 양성(Positive) 사례 중에서 모델이 올바르게 양성으로 예측한 비율
- 측정 방법: 모델이 **실제 양성인 사례 중**에서 **양성으로 예측한 비율**을 측정합니다.
- 의미: "실제 양성 중에서 얼마나 잘 잡아냈는가?"를 측정합니다. 따라서 False Negative(놓친 양성 사례)를 줄이는 데 초점을 두고 있습니다.
- 활용 목적: 중요한 사례를 놓치면 큰 문제가 되는 상황에서 중요합니다.

| 예시 | 설명 |
|:------:|:------|
| 예1 | 암 진단에서 Recall이 높으면 실제 환자를 놓칠 가능성이 적음 |
| 예2 | 광고 전환 가능성이 높은 사용자를 최대한 포괄하려면 Recall을 중시 |

---
### 3. F1 Score
- 정의: Precision과 Recall의 조화 평균
- 측정 방법: 두 지표의 조화 평균을 계산합니다.
- 의미: 정밀도와 재현율의 균형을 측정합니다.
- 활용 목적: Precision과 Recall의 중요도가 비슷하거나 균형 잡힌 모델 평가가 필요할 때 사용합니다
    - 예: 마케팅 캠페인에서 Precision(정확성)과 Recall(도달율) 사이의 균형 확인

---

### 4. ROC Curve
- 정의: 분류기의 FPR(False Positive Rate)와 TPR(True Positive Rate)을 다양한 임곗값에서 계산하여 그린 곡선
- AUC (Area Under the Curve): ROC Curve 아래의 면적으로, 모델의 분류 성능을 나타내는 척도
    - 값의 범위: 0에서 1 사이
    - 1에 가까울수록 좋은 성능
    - 0.5일 경우 랜덤 추측과 동일
- 특징
    - 데이터 불균형 문제에 덜 민감
    - 모델의 임곗값에 따른 성능 변화를 직관적으로 평가 가능

---

### 5. Accuracy (정확도)
- 정의: 전체 데이터 중에서 모델이 올바르게 예측한 비율.
- 특징:
    - 간단하고 직관적이지만 데이터의 불균형이 심한 경우에는 잘못된 성능 평가를 유발할 수 있음.
    - 예를 들어, 극도로 불균형한 데이터셋에서 대부분 음성 클래스만 존재할 경우, 모든 예측을 음성으로 하면 높은 정확도를 기록.


